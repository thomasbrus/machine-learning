{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| | |\n",
    "|---|---|\n",
    "| **Name** | Jan Ubbo van Baardewijk & Thomas Brus |\n",
    "| **Group** | ML_HMI_01 |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import operator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees\n",
    "\n",
    "In this lab, we will implement the C4.5 algorithm for classification with decision trees. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "Let's start by loading the datasets that we will be working with. The ```mushroom``` dataset is a dataset describing the features of different species of mushrooms, and the goal is to derive simple rules to predict whether a species is poisonous or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['inputs', 'targets', 'feature_names']\n",
      "22\n"
     ]
    }
   ],
   "source": [
    "# This code is given\n",
    "data = np.load(\"uci-mushrooms.npz\")\n",
    "print data.keys()\n",
    "\n",
    "d = data['inputs']\n",
    "l = data['targets']\n",
    "fname = data['feature_names']\n",
    "print len(fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions\n",
    "\n",
    "Here are a few functions to deal with categorical labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['POISONOUS', 'EDIBLE']\n",
      "['FIBROUS', 'GROOVES', 'SMOOTH', 'SCALY']\n",
      "SCALY\n"
     ]
    }
   ],
   "source": [
    "# This code is given\n",
    "\n",
    "def listValues(inputlist):\n",
    "    \"\"\"List the possible values of a given feature in a dataset\"\"\"\n",
    "    values = set([])\n",
    "    for x in inputlist:\n",
    "        values.add(x)\n",
    "    return list(values)\n",
    "\n",
    "def countLabels(labelList):\n",
    "    \"\"\"Return the most common label in a list\"\"\"\n",
    "    counts = {}\n",
    "    for x in labelList:\n",
    "        if x not in counts:\n",
    "            counts[x] = 1\n",
    "        else:\n",
    "            counts[x] += 1\n",
    "    return counts\n",
    "\n",
    "def majorityLabel(labelList):\n",
    "    counts = countLabels(labelList)\n",
    "#    print \"Majority label:\", counts\n",
    "    return max(counts.iteritems(), key=operator.itemgetter(1))[0]\n",
    "\n",
    "print listValues(l)\n",
    "print listValues(d[:,1])\n",
    "print majorityLabel(d[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing the entropy of a list of categorical values\n",
    "\n",
    "The decision tree makes its decision based on decision nodes, and these nodes are evaluated based on information gain they provide on the training data. Information gain is computed as \n",
    "$$\n",
    "IG = H(parent) - \\sum_j \\frac{N_j}{N} H(child=j)\n",
    "$$\n",
    "the entropy of the data associated with the parent, minus the weighted entropy of the data-subsets associated with the chidlren\n",
    "\n",
    "**question 1 [5 credits]]** Implement a helper function that computes the entropy of a set of categorical labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.996803828522\n"
     ]
    }
   ],
   "source": [
    "from math import log\n",
    "from __future__ import division\n",
    "\n",
    "def entropy(labelList):    \n",
    "    edibleCount = len([label for label in labelList if label == \"EDIBLE\"])\n",
    "    poisonousCount = len([label for label in labelList if label == \"POISONOUS\"])\n",
    "\n",
    "    if edibleCount == 0 or poisonousCount == 0: return 0\n",
    "\n",
    "    ediblePercentage = edibleCount / len(labelList)\n",
    "    poisonousPercentage = poisonousCount / len(labelList)\n",
    "\n",
    "    return -ediblePercentage * log(ediblePercentage, 2) - poisonousPercentage * log(poisonousPercentage, 2)\n",
    "\n",
    "print entropy(l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing the tree\n",
    "\n",
    "Below, you are given the code to make a decision tree, train it and classify datapoints using binary decision nodes based on categorical variables. The implementation includes  ```Leaf``` nodes, wich return a classification result, and ```BinaryNode```, which decide which child should be responsible for the further classification.\n",
    "\n",
    "The ```BinaryNode``` constructor selects the most informative value in the data it's trained on, and makes a decision based on whether or not the feature of the datapoint to be classified equals that value, or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This code is given\n",
    "\n",
    "class Node:\n",
    "    def classify(self, datapoint):\n",
    "        \"\"\"Return the classification label for the given datapoint\"\"\"\n",
    "        raise Exception(\"Not Implemented\")        \n",
    "    def entropy(self):\n",
    "        \"\"\"Return the entropy of the training data associated with this node\"\"\"\n",
    "    def informationGain(self):\n",
    "        \"\"\"Return the information gain of this node on the training data \"\"\"\n",
    "        raise Exception(\"Not Implemented by derived class\")\n",
    "\n",
    "class Leaf(Node):\n",
    "    def __init__(self,inputs, targets):\n",
    "        self.v = majorityLabel(targets)\n",
    "        self.inputs = inputs\n",
    "        self.targets = targets\n",
    "        self.children = None\n",
    "        self.H = entropy(targets)\n",
    "        \n",
    "    def entropy(self):\n",
    "        return self.H\n",
    "    \n",
    "    def informationGain(self):\n",
    "        return 0.\n",
    "    \n",
    "    def classify(self,datapoint):\n",
    "        return self.v\n",
    "    \n",
    "    def __str__(self):\n",
    "        return \"\\t\\t\\t=> %s\" % (self.v)\n",
    "    \n",
    "class BinaryNode(Node):\n",
    "    \"\"\"Make a decision based on whether a given feature equals a given value, or not\"\"\"\n",
    "    def __init__(self, inputs, targets, feature):\n",
    "        self.feature = feature\n",
    "        self.value   = None\n",
    "        self.IG      = 0.0\n",
    "        self.H       = entropy(targets)                     # Entropy of unsplit data\n",
    "        self.inputs  = inputs\n",
    "        self.targets = targets\n",
    "        N = float(targets.size)                             # Total number of datapoints\n",
    "\n",
    "        for value in listValues(inputs[:,feature]):\n",
    "            indicesTrue  = inputs[:,feature]==value\n",
    "            indicesFalse = inputs[:,feature]!=value\n",
    "\n",
    "            if indicesTrue.all() or indicesFalse.all():\n",
    "                continue\n",
    "\n",
    "            children = [ Leaf(inputs[indicesTrue,:], targets[indicesTrue]),\n",
    "                         Leaf(inputs[indicesFalse,:], targets[indicesFalse]) ]\n",
    "            conditions = [ \"%s==%s\" % (fname[feature], value), \"%s!=%s\" % (fname[feature], value) ]\n",
    "\n",
    "            Nt = float(children[0].targets.size)        # N.o. datapoints in True condition\n",
    "            Nf = float(children[1].targets.size)        # N.o. datapoints in False condition\n",
    "            pt = Nt/N\n",
    "            pf = Nf/N\n",
    "            ig = self.H - pt * children[0].entropy() - pf * children[1].entropy()\n",
    "            \n",
    "            if ig > self.IG:\n",
    "                self.value = value\n",
    "                self.IG = ig\n",
    "                self.children = children\n",
    "                self.conditions = conditions\n",
    "    \n",
    "    def entropy(self):\n",
    "        return self.H\n",
    "    \n",
    "    def informationGain(self):\n",
    "        \"\"\"Return the entropy gain of splitting the given data according to \n",
    "        the specified feature and value\"\"\"\n",
    "        return self.IG\n",
    "    \n",
    "    def classify(self, x):\n",
    "        if x[self.feature] == self.value:\n",
    "            return self.children[0].classify(x)\n",
    "        else:\n",
    "            return self.children[1].classify(x)\n",
    "        \n",
    "    def __str__(self):\n",
    "        return \" => Binary decision: '%s =?= %s'\" % (fname[self.feature], self.value )\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This code is given\n",
    "\n",
    "def c45(inputs, targets, minSize=0):\n",
    "    ig = 0.\n",
    "    res = None\n",
    "    for feature in range(inputs.shape[1]):\n",
    "        n = BinaryNode(inputs,targets,feature)\n",
    "        if n.informationGain() > ig:\n",
    "            ig = n.informationGain()\n",
    "            res = n\n",
    "    if not res:\n",
    "        print \"WARNING: No informative feature found (Fishy!)\"\n",
    "        return res\n",
    "    for i in range(len(res.children)):\n",
    "        if res.children[i].entropy() > 0. and res.children[i].targets.size>minSize:\n",
    "            c = c45(res.children[i].inputs, res.children[i].targets, minSize)\n",
    "            if c:\n",
    "                res.children[i]=c\n",
    "    return res\n",
    "\n",
    "root = c45(d,l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+->   => Binary decision: 'odour =?= NONE'\n",
      "   +-> odour==NONE  => Binary decision: 'spore-print-colour =?= GREEN'\n",
      "   |  +-> spore-print-colour==GREEN \t\t\t=> POISONOUS\n",
      "   |  +-> spore-print-colour!=GREEN  => Binary decision: 'stalk-surface-below-ring =?= SCALY'\n",
      "   |     +-> stalk-surface-below-ring==SCALY  => Binary decision: 'gill-size =?= BROAD'\n",
      "   |     |  +-> gill-size==BROAD \t\t\t=> EDIBLE\n",
      "   |     |  +-> gill-size!=BROAD \t\t\t=> POISONOUS\n",
      "   |     +-> stalk-surface-below-ring!=SCALY  => Binary decision: 'cap-surface =?= GROOVES'\n",
      "   |        +-> cap-surface==GROOVES \t\t\t=> POISONOUS\n",
      "   |        +-> cap-surface!=GROOVES  => Binary decision: 'gill-size =?= BROAD'\n",
      "   |           +-> gill-size==BROAD \t\t\t=> EDIBLE\n",
      "   |           +-> gill-size!=BROAD  => Binary decision: 'bruises? =?= BRUISES'\n",
      "   |              +-> bruises?==BRUISES \t\t\t=> POISONOUS\n",
      "   |              +-> bruises?!=BRUISES \t\t\t=> EDIBLE\n",
      "   +-> odour!=NONE  => Binary decision: 'bruises? =?= BRUISES'\n",
      "      +-> bruises?==BRUISES  => Binary decision: 'stalk-root =?= CLUB'\n",
      "      |  +-> stalk-root==CLUB \t\t\t=> EDIBLE\n",
      "      |  +-> stalk-root!=CLUB  => Binary decision: 'stalk-root =?= ROOTED'\n",
      "      |     +-> stalk-root==ROOTED \t\t\t=> EDIBLE\n",
      "      |     +-> stalk-root!=ROOTED  => Binary decision: 'gill-spacing =?= CLOSE'\n",
      "      |        +-> gill-spacing==CLOSE \t\t\t=> POISONOUS\n",
      "      |        +-> gill-spacing!=CLOSE \t\t\t=> EDIBLE\n",
      "      +-> bruises?!=BRUISES \t\t\t=> POISONOUS\n"
     ]
    }
   ],
   "source": [
    "# This code is given\n",
    "\n",
    "def prettyPrint(tree, cond=\"\", dlist = [True]):\n",
    "    indent = \"\"\n",
    "    for d in dlist[:-1]:\n",
    "        indent += \"   \" if d else \"|  \"\n",
    "    print indent + \"+-> \" + cond + \" \" + tree.__str__()\n",
    "    if tree.children:\n",
    "        for cond,cld in zip(tree.conditions[:-1],tree.children[:-1]):\n",
    "            prettyPrint(cld,cond,dlist + [ False ])\n",
    "        prettyPrint(tree.children[-1], tree.conditions[-1], dlist+[ True ])\n",
    "\n",
    "prettyPrint(root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training \n",
    "\n",
    "The code above trains a decision tree.\n",
    "\n",
    "**Question 2 [5 credits]** Split the ```mushroom``` dataset into 10 folds and perform 10-fold cross-validated training and testing of the decision tree. Report de tree's performance in terms of average precision, recall and accuracy.\n",
    "\n",
    "**Question 3 [5 credits]** Modify the training function provided above to stop splitting nodes when the corresponding training data contains less than ```minSize``` datapoints, where ```minSize``` is a parameter to the function. Split the data into 80\\% train and 20\\% test, and use 10-fold cross-validation on the training set to find a good value for this parameter. Then report the confusion matrix, precision, recall and accuracy that you obtain on the test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "def calculate_precision(decision_tree, test_inputs, test_labels):\n",
    "    classifications = [decision_tree.classify(test_input) for test_input in test_inputs]\n",
    "    true_positives = [actual == \"EDIBLE\" and expected == \"EDIBLE\" for actual, expected in zip(classifications, test_labels)].count(True)\n",
    "    false_positives = [actual == \"POISONOUS\" and expected == \"EDIBLE\" for actual, expected in zip(classifications, test_labels)].count(True)\n",
    "    return true_positives / (true_positives + false_positives)\n",
    "\n",
    "def calculate_recall(decision_tree, test_inputs, test_labels):\n",
    "    classifications = [decision_tree.classify(test_input) for test_input in test_inputs]\n",
    "    true_positives = [actual == \"EDIBLE\" and expected == \"EDIBLE\" for actual, expected in zip(classifications, test_labels)].count(True)\n",
    "    false_negatives = [actual == \"EDIBLE\" and expected == \"POISONOUS\" for actual, expected in zip(classifications, test_labels)].count(True)\n",
    "    return true_positives / (true_positives + false_negatives)\n",
    "\n",
    "def calculate_accuracy(decision_tree, test_inputs, test_labels):\n",
    "    total = len(test_inputs)\n",
    "    classifications = [decision_tree.classify(test_input) for test_input in test_inputs]\n",
    "    correct = [actual == expected for actual, expected in zip(classifications, test_labels)].count(True)\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def traverseLeaves(node):\n",
    "    if isinstance(node, Leaf):\n",
    "        return [node]\n",
    "    else:\n",
    "        return traverseLeaves(node.children[0]) + traverseLeaves(node.children[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Answer to Q2\n",
    "\n",
    "def train_and_test(inputs, labels, folds=10, test_sample=0):\n",
    "    test_set_length = int(len(inputs) / folds)\n",
    "    \n",
    "    training_inputs = np.concatenate((inputs[:test_set_length*test_sample], inputs[test_set_length*(test_sample+1):]), axis=0)\n",
    "    training_labels = np.concatenate((labels[:test_set_length*test_sample], labels[test_set_length*(test_sample+1):]), axis=0)\n",
    "    \n",
    "    test_inputs = inputs[test_set_length*test_sample:test_set_length*(test_sample+1)]\n",
    "    test_labels = labels[test_set_length*test_sample:test_set_length*(test_sample+1)]\n",
    "        \n",
    "    decision_tree = c45(training_inputs, training_labels, minSize=100)\n",
    "        \n",
    "    accuracy = calculate_accuracy(decision_tree, test_inputs, test_labels)\n",
    "    precision = calculate_precision(decision_tree, test_inputs, test_labels)\n",
    "    recall = calculate_recall(decision_tree, test_inputs, test_labels)\n",
    "    \n",
    "    return (accuracy, precision, recall)\n",
    "\n",
    "results = [train_and_test(d, l, folds=10, test_sample=i) for i in range(10)]\n",
    "\n",
    "print \"Accuracy\", np.mean([result[0] for result in results])\n",
    "print \"Precision\", np.mean([result[1] for result in results])\n",
    "print \"Recall\", np.mean([result[2] for result in results])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# Answer to Q3\n",
    "\n",
    "def calculate_optimal_min_size(inputs, targets):\n",
    "    # TODO: Do something with confidence interval theory?\n",
    "    # https://books.google.nl/books?id=VoWIIOKVzR4C&pg=PA61&lpg=PA61&dq=c45+lower+bound+number&source=bl&ots=5w78TFMwKJ&sig=zmSVSewBPKd01wVMwl7r0O8kAoA&hl=nl&sa=X&ved=0ahUKEwjsw8eLo8rLAhUH7g4KHWm_D0MQ6AEIOTAE#v=onepage&q=c45%20lower%20bound%20number&f=false\n",
    "    return len(inputs) * 0.005\n",
    "\n",
    "training_set_length = int(len(d) * 0.8)\n",
    "\n",
    "training_inputs = d[0:training_set_length]\n",
    "training_labels = l[0:training_set_length]\n",
    "\n",
    "test_inputs = d[training_set_length:]\n",
    "test_labels = l[training_set_length:]\n",
    "\n",
    "min_size = calculate_optimal_min_size(training_inputs, training_labels)\n",
    "decision_tree = c45(training_inputs, training_labels, minSize=min_size)\n",
    "\n",
    "print calculate_accuracy(decision_tree, test_inputs, test_labels)\n",
    "print calculate_precision(decision_tree, test_inputs, test_labels)\n",
    "print calculate_recall(decision_tree, test_inputs, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Different types of decisions\n",
    "\n",
    "Our tree for now can only make decisions based on binary conditions, but our dataset contains features with more than two possible values. \n",
    "\n",
    "**Question 4 [10 credits]** Implement a new type of node, ```MultiNode``` that has a child per possible value of the feature. Be careful to deal with the possibility that the test data may contain categories that the training set didn't have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-108-28b10f9514c0>, line 10)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-108-28b10f9514c0>\"\u001b[0;36m, line \u001b[0;32m10\u001b[0m\n\u001b[0;31m    def entropy(self):\u001b[0m\n\u001b[0m      ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "# This code must be modified\n",
    "\n",
    "class MultiNode(Node):\n",
    "    \"\"\"Make a decision based the value of the given feature, i.e. as many children as there are \n",
    "    possible values for the feature\"\"\"\n",
    "    def __init__(self, inputs, targets, feature):\n",
    "        # YOUR CODE COMES HERE\n",
    "        \n",
    "                \n",
    "    def entropy(self):\n",
    "        return self.H\n",
    "    \n",
    "    def informationGain(self):\n",
    "        \"\"\"Return the entropy gain of splitting the given data according to \n",
    "        the specified feature and value\"\"\"\n",
    "        return self.IG\n",
    "    \n",
    "    def classify(self, x):\n",
    "        # YOUR CODE COMES HERE\n",
    "        \n",
    "    def __str__(self):\n",
    "        return \"=> n-ary decision: %s = ?\" % (fname[self.feature]) # You may want to change this to suit your own implementation\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with heterogeneous features\n",
    "\n",
    "The code below keeps a list of the type of node that is applicable for each feature, in this case ```BinaryNode``` or ```Multinode```. \n",
    "\n",
    "**Question 5 [5 credits]** Is a tree of multinodes better than a tree of binary nodes? Is it more compact? Is it generalising better?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Answer to Q5*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#This code is given but minSize is not implemented\n",
    "\n",
    "featureType = [ MultiNode for i in range(l.size) ]\n",
    "\n",
    "def c45(inputs, targets, minSize, featureType):\n",
    "    ig = 0.\n",
    "    res = None\n",
    "    for feature in range(inputs.shape[1]):\n",
    "        n = featureType[feature](inputs,targets,feature)\n",
    "        if n.informationGain() > ig:\n",
    "            ig = n.informationGain()\n",
    "            res = n\n",
    "    for i in range(len(res.children)):\n",
    "        if res.children[i].entropy() > 0.:\n",
    "            res.children[i] = c45(res.children[i].inputs, res.children[i].targets, minSize, featureType)\n",
    "    return res\n",
    "\n",
    "root = c45(d,l,5, featureType)\n",
    "prettyPrint(root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Playground for answering Q5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuous values\n",
    "\n",
    "Until now, we have only dealt with categorical features. In the next part of the lab, you will deal with mixed categorical and continuous features.\n",
    "\n",
    "One way to deal with continuous features is to split the dataset based on whether the feature is larger than a certain threshold, or not. The number of possible threshold values to consider is, of course, infinite in theory, but when deciding what the threshold value should be, based on a training dataset, only threshold values that will result in a different classification of those training examples matter: those are values that are smaller than one datapoint's feature value and larger than another's. Moreover, when considering a threshold in between two feature values, any value of the threshold in that range will result in the same classification, but to maximise generalisation it makes sense to maximise the margin: that is, to take a threshold value that is in the middle between the two.\n",
    "\n",
    "Also note that it only makes sense to consider ranges of feature values that are disjoint, i.e., consider thresholds that are in the middle between neighbouring datapoints (along that feature's dimension), not in the middle between any pair of datapoints.\n",
    "\n",
    "**Question 6 [10 credits]**. You guessed it, implement a node ```ContNode``` that selects a threshold value based on its training subset and classifies new datapoints based on whether they are smaller than that value, or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This code must be modified.\n",
    "\n",
    "class ContNode(Node):\n",
    "    \"\"\"Make a binary decision based on a threshold, for the value of the given feature\"\"\"\n",
    "    def __init__(self, inputs, targets, feature):\n",
    "         # YOUR CODE COMES HERE\n",
    "                \n",
    "    def entropy(self):\n",
    "        return self.H\n",
    "    \n",
    "    def informationGain(self):\n",
    "        \"\"\"Return the entropy gain of splitting the given data according to \n",
    "        the specified feature and value\"\"\"\n",
    "        return self.IG\n",
    "    \n",
    "    def classify(self, x):\n",
    "        if x[self.feature] >= self.value:\n",
    "            return self.children[0].classify(x)\n",
    "        else:\n",
    "            return self.children[1].classify(x)\n",
    "        \n",
    "    def __str__(self):\n",
    "        return \"=> threshold decision: %s >= %f\" % (fname[self.feature],self.value)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification based on mixed features\n",
    "\n",
    "The following dataset contains 14 features describing people's situation, and the classification task is to predict whether they earn more than \\$50k a year or not. The categorical decision nodes we implemented above can work with any representation of categorical data, but the continuous decision node requires a numerical representation of the feature. Numpy arrays cannot easily contain both strings and numbers, and so to simplify our life, we converted the dataset to \n",
    "\n",
    "To allow our classifier to work with mixed features, we use a numerical representation of the categorical features in the following. So, let's first load the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This code is given\n",
    "\n",
    "income = np.load('income.npz')\n",
    "print income.keys()\n",
    "inputs = income['inputs']\n",
    "targets = income['targets']\n",
    "fname = income['fnames']\n",
    "ftypes = income['ftypes']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 7 [10 credits]** Take 1000 training datapoints at random from the dataset, and another 1000 test points that are distinct from those (don't mix train and test). Train a decision tree on the train set and report the confusion matrix obtained on the test set. What are the obtained precision, recall and accuracy? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This code is given\n",
    "\n",
    "featureType = [ ContNode if ftypes[i]=='continuous' else BinaryNode for i in range(len(ftypes)) ]\n",
    "tree = c45(inputs[:10000,:],targets[:10000],100,featureType)\n",
    "prettyPrint(tree)\n",
    "\n",
    "# Answer to Q7\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
