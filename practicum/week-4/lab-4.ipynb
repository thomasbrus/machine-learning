{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| | |\n",
    "|---|---|\n",
    "| **Name** | Jan Ubbo van Baardewijk & Thomas Brus |\n",
    "| **Group** | ML_HMI_01 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\newcommand{\\x}{\\mathbf{x}}\n",
    "\\newcommand{\\w}{\\mathbf{w}}\n",
    "\\newcommand{\\z}{\\mathbf{z}}\n",
    "\\newcommand{\\wx}{\\w^\\top\\x}\n",
    "\\newcommand{\\dataset}{\\mathbf{X}}\n",
    "\\newcommand{\\tset}{t\\negthickspace t}\n",
    "$\n",
    "\n",
    "# Week 4: Neural networks\n",
    "\n",
    "The basic unit of a multi-layer perceptron (MLP) is the neuron, which is a computational unit that computes a weighted sum of its inputs $\\wx$ and pushes this through a non-linear activation function. Important is that this activation should be differentiable, as we will train the weights of the network by gradient descent. This requires that we should be able to compute the gradient of an error function with respect to the weights. \n",
    "\n",
    "In a sense, this whole lab is about taking derivatives and applying the chain rule of derivation. As a reminder, the rule is that the derivative of a function $f$ applied to the result of a function $g$ $f(g(x))$ can be decomposed as:\n",
    "$$\n",
    "\\frac{\\partial}{\\partial x} f(g(x)) = \\frac{\\partial f(g(x))}{\\partial g(x)} \\, \\frac{\\partial g(x)}{\\partial x} \n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import copy\n",
    "\n",
    "maxit=100\n",
    "def gradDesc(w, errorfn, gradfn,  eta=1e-3, verbose=True):\n",
    "    err = errorfn(w)             # Compute the error function\n",
    "    errs = np.zeros(maxit)       # Keep track of the error function\n",
    "    ts = np.zeros(maxit)         # Keep track of time stamps\n",
    "    start = time.time()\n",
    "\n",
    "    newError = errorfn(w) + 1.\n",
    "    for n in xrange(maxit):\n",
    "        grad = gradfn(w)\n",
    "        if verbose:\n",
    "            print \"##\", n, \"err:\", newError, \"eta:\", eta, \"w:\", w\n",
    "            print \"  Gradient:\", grad\n",
    "            # print \"  FD grad: \", gradfd(w,errorfn)\n",
    "        pw = copy.copy(w)\n",
    "        w -= grad * eta     # Update the weights\n",
    "        \n",
    "        pastError = newError\n",
    "        newError = errorfn(w)\n",
    "        while eta > 0. and pastError - newError < 0: # If the error increases, eta is too large\n",
    "            eta /= 2.0                                # halve it, and\n",
    "            w = pw - eta * grad                      # try again from the original value of the weights\n",
    "            newError = errorfn(w)\n",
    "        else:                                        # If the error goes down,\n",
    "            eta *= 1.2                               # try to increase eta a little, to speed up things.\n",
    "                \n",
    "        if pastError-newError < 1e-5:                # If we couldn't decrease the error anymore, \n",
    "            return w, errs[:n], ts[:n]               # just give up\n",
    " \n",
    "        errs[n] = newError                           # Keep track of how the errors evolved\n",
    "        ts[n] = time.time()-start\n",
    "\n",
    "    return w, errs, ts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neuron definition\n",
    "\n",
    "In this case, let's set up our network to have the sigmoidal activation function that we've seen before in logistic regression, \n",
    "$$\n",
    "\\sigma(a) = \\frac{1}{1+e^{-a}}\\ ,\n",
    "$$ \n",
    "so that the activation $z_i$ of neuron $i$ is given by\n",
    "$$\n",
    "z_i = \\sigma(\\w_i^\\top\\x_i) \\ ,\n",
    "$$\n",
    "where $\\x_i$ is the input to neuron $i$. Other activation functions are possibble, such as a linear function ($z=\\wx$) or a hyperbolic tangend ($z=\\tanh(\\wx)$). Let's consider a network of a single neuron, then the output of our network $y(\\w,\\x) = \\sigma(\\wx)$\n",
    "\n",
    "## Training\n",
    "\n",
    "To train a single neuron, we have a set of datapoints $\\dataset = \\{ \\x_1\\dots \\x_N\\}$ and corresponding targets $\\tset = \\{ t_1 \\dots t_N \\}$. We use this dataset to find optimal weights, and we define optimality on terms of an error function. Let's define our error function as a sum of squared errors:\n",
    "$$\n",
    "E(\\w) = \\tfrac{1}{2} \\sum_{n=1}^N (y(\\w,\\x_n) - t_n)^2\n",
    "$$\n",
    "\n",
    "**Question [5 credits]**: Notice how, although our single neuron is very similar to the logistic regression we saw in previous weeks, our error function is now quite different. What is now the error for our set $\\dataset, \\tset$? Write it out algebraically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "E(\\w) = \\tfrac{1}{2} \\sum_{n=1}^N (\\sigma(\\wx_n) - t_n)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question [5 credits]** What is the gradient of this error with respects to the weights of the neuron $\\w$? Be careful, this is not the same as what we saw in the lecture, where the output neuron had a linear activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Answer to Q2*\n",
    "\n",
    "$$\n",
    "\\frac{\\partial E}{\\partial w_{ij}} = \\frac{\\partial E}{\\partial z_j} \\frac{\\partial z_j}{\\partial\\mathrm{a_j}} \\frac{\\partial \\mathrm{a_j}}{\\partial w_{ij}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathrm{a_j}}{\\partial w_{ij}} = \\frac{\\partial}{\\partial w_{ij}}\\left(\\sum_{k=1}^{n}w_{kj}x_k\\right) = x_i\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial z_j}{\\partial\\mathrm{a_j}} = \\frac {\\partial}{\\partial \\mathrm{a_j}}\\sigma(\\mathrm{a_j}) = \\sigma(\\mathrm{a_j})(1-\\sigma(\\mathrm{a_j}))\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial E}{\\partial z_j} = \\frac{\\partial E}{\\partial y} = \\frac{\\partial}{\\partial y} \\frac{1}{2}(t - y)^2 = y - t\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial E}{\\partial w_{ij}} = (y - t) \\sigma(\\mathrm{a_j})(1-\\sigma(\\mathrm{a_j})) x_i = (y - t) z_j (1 - z_j)) x_i = (y - t) y (1 - y)) x_i\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question [10 credits]** If now we had a neural network with two neurons in a hidden layer, \n",
    "* what would be the error function, analytically?\n",
    "\n",
    "$$\n",
    "E(\\w) = \\tfrac{1}{2} \\sum_{n=1}^N (y(\\w,\\x_n)- t_n)^2\n",
    "$$\n",
    "\n",
    "where\n",
    "$\n",
    "y(\\w,\\x_n) = \\sigma( \\sum_{j=1}^2 w^{(2)}_{1j} \\sigma( \\sum_{i=1}^D w^{(1)}_{ji} x_i + w^{(1)}_{j0}) + w^{(2)}_{10})\n",
    "$\n",
    "\n",
    "* What is the derivative of that error with respect to the weights of the first and second layer?\n",
    "\n",
    "$$\n",
    "\\frac{\\partial E_n}{\\partial w_{ji}} = \\delta_j z_i \\tag*{(5.53)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\delta_j = h'(a_j) \\sum_{k} \\w_{kj} \\delta_k \\tag*{(5.56)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\delta_j = \\sigma(\\mathrm{a_j})(1-\\sigma(\\mathrm{a_j})) \\sum_{k} \\w_{kj} \\delta_k\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\delta_k = y_k - t_k \\tag*{(5.54)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\delta_j = \\sigma(\\mathrm{a_j})(1-\\sigma(\\mathrm{a_j})) \\sum_{k} \\w_{kj} (y_k - t_k)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial E_n}{\\partial w_{ji}} = \\sigma(\\mathrm{a_j})(1-\\sigma(\\mathrm{a_j})) \\sum_{k} \\w_{kj} (y_k - t_k) z_i\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial E_n}{\\partial w_{ji}} = y(1-y) \\sum_{k} \\w_{kj} (y_k - t_k) z_i\n",
    "$$\n",
    "\n",
    "* How do your equations correspond to the quantities computed in backpropagation?\n",
    "\n",
    "..?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Answer to Q3*\n",
    "\n",
    "(see above)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question [10 credits]** Implement the following python functions: \n",
    "\n",
    "* `error(outputs, targets)`, which implements the sum-squared error on our dataset, and a function \n",
    "* `gradError(outputs,targets)`, which computes the gradient of that error with respect to its input variable `outputs`\n",
    "* `neuron(weights, inputs)`, wich implements a neuron with sigmoidal activation function, weights $\\w$ and inputs $\\x$. Decide how you deal with the bias (separately, or by augmenting the input data)\n",
    "* `gradNeuron(weights, inputs)`, which implements the gradient of the neuron's output with respect to the neuron's weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "maxexp = 40\n",
    "\n",
    "def sigma(a):\n",
    "    '''Logistic function, avoiding warnings of numerical overflows'''\n",
    "    if a > maxexp:\n",
    "        return 1.\n",
    "    if a <- maxexp:\n",
    "        return 0.\n",
    "    return 1. / (1. + np.exp(-a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error(outputs, targets) =  0.0\n",
      "gradError(outputs, targets) =  0.0\n",
      "neuron(weights, inputs) =  0.425557483188\n",
      "gradNeuron(weights, inputs) =  0.244458311691\n"
     ]
    }
   ],
   "source": [
    "# Answer to Q4\n",
    "\n",
    "def error(outputs, targets):\n",
    "    return 0.5 * sum([(o - t)**2 for o, t in zip(outputs, targets)])\n",
    "\n",
    "print \"error(outputs, targets) = \", error(outputs, targets)\n",
    "\n",
    "def gradError(outputs, targets):\n",
    "    return sum([(o - t) for o, t in zip(outputs, targets)])\n",
    "\n",
    "print \"gradError(outputs, targets) = \", gradError(outputs, targets)\n",
    "\n",
    "def neuron(weights, inputs, activationFn=sigma):\n",
    "    bias, weights = weights[0], weights[1:]\n",
    "    return activationFn(bias + sum([w * x for w, x in zip(weights, inputs)]))\n",
    "\n",
    "print \"neuron(weights, inputs) = \", neuron(weights, inputs)\n",
    "\n",
    "def gradNeuron(weights, inputs):\n",
    "    return neuron(weights, inputs) * (1 - neuron(weights, inputs))\n",
    "\n",
    "print \"gradNeuron(weights, inputs) = \", gradNeuron(weights, inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Question [10 credits]** Verify using finite differences that the gradients you implemented for the above functions is actually correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated gradient 0.65\n",
      "Actual gradient 0.6\n",
      "Estimated gradient 0.249947929684\n",
      "Actual gradient 0.25\n"
     ]
    }
   ],
   "source": [
    "## Answer Q5\n",
    "\n",
    "targets = [0, 1]\n",
    "\n",
    "delta = 0.05\n",
    "outputs, outputs_delta = [0.8, 0.8], [0.8 + delta, 0.8 + delta]\n",
    "\n",
    "estimated_gradient = (error(outputs_delta, targets) - error(outputs, targets)) / delta\n",
    "actual_gradient = gradError(outputs, targets)\n",
    "\n",
    "print \"Estimated gradient\", estimated_gradient\n",
    "print \"Actual gradient\", actual_gradient\n",
    "\n",
    "inputs = [1, 0]\n",
    "\n",
    "delta = 0.05\n",
    "weights, weights_delta = [-0.5, 0.5, -0.5], [-0.5, 0.5 + delta, -0.5 + delta]\n",
    "\n",
    "estimated_gradient = (neuron(weights_delta, inputs) - neuron(weights, inputs)) / delta\n",
    "actual_gradient = gradNeuron(weights, inputs)\n",
    "\n",
    "print \"Estimated gradient\", estimated_gradient\n",
    "print \"Actual gradient\", actual_gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a single neuron\n",
    "\n",
    "We now have the code to implement gradient descent on a single neuron. We simply need to apply the chain rule of derivation to get the complete gradient.\n",
    "** Question [10 marks] ** \n",
    "Implement the error function and the gradient of the error function with respect to the weights. Then use last week's code to perform gradient descent on the data provided. Plot the evolution of the error in one plot, and the function that the neuron implements overlayed with the training data (in another plot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = np.load(\"curve.npz\")\n",
    "train = np.array([data['train'][:,0]]).T\n",
    "traint = data['train'][:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX8AAAEACAYAAABbMHZzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAvT/AdEg50D1LA/0AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10a8a17d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAEACAYAAABVtcpZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nhYCISMAUAiIiAft/r9DwgYzlmPAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10a9bf6d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Answer\n",
    "\n",
    "def errorFn(weights):\n",
    "    outputs = [neuron(weights, inputs) for inputs in train]\n",
    "    return error(outputs, traint)\n",
    "\n",
    "def gradErrorFn(weights):\n",
    "    outputs = [neuron(weights, inputs) for inputs in train]\n",
    "    return gradError(outputs, traint)\n",
    "\n",
    "weights, errors, timestamps = gradDesc([0, 0], errorFn, gradErrorFn,  eta=1e-3, verbose=False)\n",
    "\n",
    "plt.plot(timestamps, errors, 'blue', label='error')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "colours = [['b', 'r'][int(x)] for x in np.nditer(traint)] \n",
    "\n",
    "plt.scatter(train, traint, 20, colours)\n",
    "\n",
    "xs = np.arange(-0.25, 1.25, 0.05)\n",
    "ys = [neuron(weights, [x]) for x in xs]\n",
    "\n",
    "plt.plot(xs, ys, 'blue', label='error')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Defining and training a network\n",
    "\n",
    "We define a feed-forward neural network using the following information:\n",
    "1. The number of layers in the network\n",
    "1. The number of nodes in each layer\n",
    "1. The activation function used in each layer (notice that all nodes in a layer use the same activation function)\n",
    "\n",
    "The important thing to notice in such a network, is that the input of the nodes in layer $n$ are the outputs of the nodes in layer $n-1$, so that we have a chaining of functions and the chain rule of derivatives applies directly. We can, therefore, apply the chain rule mechanically and compute the gradient using a combination of the forward-propagated inputs and backward-propagated errors.\n",
    "\n",
    "The code below implements a neural network with arbitrary numbers of hidden nodes, arbitrary numbers of hidden layers, and arbitrary activation functions. Do look at the code and make sure you understand how it works, and how to use it. To use it, activation functions need to be defined. The following cell provides you with two examples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This code is given\n",
    "# The following functions are activation functions. For efficiency, they combine the computation of \n",
    "# the activation function itself and its gradient wrt its input values, as both often rely on computing \n",
    "# the same intermediate quantities\n",
    "\n",
    "def ssegf(y,t):\n",
    "    '''Error and gradient function for sum-of-squares'''\n",
    "    return .5*(y-t)**2, (y-t)\n",
    "\n",
    "def linegf(a):\n",
    "    '''Error and gradient function for linear activation'''\n",
    "    return a, np.ones(a.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question [10 credits]**: Implement the logistic (sigmoidal) and the hyperbolic tangent \"Error and Gradient\" activation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Answer\n",
    "\n",
    "def logistic_error_and_gradient(a):\n",
    "    return sigma(a), sigma(a) * (1 - sigma(a))\n",
    "\n",
    "def tangent_error_and_gradient(a):\n",
    "    return tanh(a), "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class NN:\n",
    "    '''Implementation of an artificial neural network'''\n",
    "    def __init__(self, nIn, nHidden, nOut, actFn):\n",
    "        '''Initialise the network with random weights and biases. Parameters are:\n",
    "        nIn: Number of input nodes (dimensionality of the data)\n",
    "        nHidden: LIST of numbers of hidden nodes, one integer per layer. Empty list for no hidden layers\n",
    "        nOut: Number of output nodes; dimensionality of the targets\n",
    "        actFn: Tuple of activation functions; one function per computational layer (hidden or output)'''\n",
    "        \n",
    "        assert(len(nHidden) == len(actFn)-1)    # Check that the parameters are consistent\n",
    "        \n",
    "        self.numNodes = [nIn]                   # Create a list of numbers of nodes per layer\n",
    "        if nHidden:\n",
    "            self.numNodes.extend(nHidden)\n",
    "        self.numNodes.append(nOut)\n",
    "\n",
    "        # Initialise weights and biases\n",
    "        self.w = [ 1e-0 * np.random.randn(self.numNodes[i],self.numNodes[i-1]) for i in range(1,len(self.numNodes)) ]\n",
    "        # print \"Weights\", self.w\n",
    "        self.biases = [ 1e-0 * np.random.randn(self.numNodes[i]) for i in range(1,len(self.numNodes))]\n",
    "        # print \"Biases:\", self.biases\n",
    "        self.actfn = actFn\n",
    "        \n",
    "    def forward(self,x):\n",
    "        ''' Compute the activations of the nodes of each layer.\n",
    "            For efficiency, use \"EGF\" functions and store the gradient of the activation function as well. In\n",
    "            other words each entry of self.fw is a tuple containing (z_i, h'(a_i))\n",
    "        '''\n",
    "        self.fw = [ self.actfn[0](self.w[0].dot(x) + self.biases[0]) ]  # First layer takes data as input\n",
    "        for h,w,b in zip(self.actfn[1:],self.w[1:],self.biases[1:]):    # following layers take the output of  \n",
    "            self.fw.append(h(w.dot(self.fw[-1][0]) + b))                #    the previous layer\n",
    "        return self.fw[-1][0]\n",
    "    \n",
    "    def back(self, errorgrad):\n",
    "        '''Implement backpropagation of the errors'''\n",
    "        self.delta = [ self.fw[-1][1] * errorgrad ]             # Start with the gradient of the error function\n",
    "        \n",
    "        for layer in range(len(self.w)-1,0,-1):\n",
    "            hprime = self.fw[layer-1][1]                        # h'(a_j)\n",
    "            delta = self.delta[0]                               # list gets extended at start, first elt: delta_k\n",
    "            w = self.w[layer]                                   # w_kj\n",
    "\n",
    "            self.delta.insert(0,hprime * (self.delta[0].dot(w))) # Use dot product to sum over all FOLLOWING weights        \n",
    "\n",
    "    def gradients(self, x, t, eta=1e-3):\n",
    "        '''Compute the gradients of the error of a single datapoint, with respect to\n",
    "        all weights and all biases'''\n",
    "        y = self.forward(x)                        # Forward propagation and computation of h'\n",
    "        error = y-t\n",
    "        self.back(error)                           # Backward propagation\n",
    "                              \n",
    "        gradw = []\n",
    "        gradb = []\n",
    "        gradw.append(np.outer(self.delta[0],x))    # derivative wrt w_ji = delta_j z_i, where z_i == x_i for input nodes\n",
    "        gradb.append(self.delta[0])                #    and z_i == 1 for bias nodes\n",
    "        \n",
    "        for i in range(1,len(self.w)):\n",
    "            gradw.append(np.outer(self.delta[i],self.fw[i-1][0])) # for folloing layers, use output of previous layer\n",
    "            gradb.append(self.delta[i])\n",
    "        return gradw,gradb,.5*error*error          # Return gradient wrt w and bias, and the error\n",
    "\n",
    "    def batchGradients(self, data,targets):\n",
    "        '''Compute the gradients of all weights and biases, for the complete dataset'''                      \n",
    "        gradw,gradb,error = self.gradients(data[0,:],targets[0])\n",
    "        for x,t in zip(data[1:,:],targets[1:]):\n",
    "            tw,tb,e = self.gradients(x,t)\n",
    "            error += e\n",
    "            for i in range(len(gradw)):\n",
    "                gradw[i] += tw[i]\n",
    "                gradb[i] += tb[i]\n",
    "        return gradw,gradb,error\n",
    "    \n",
    "    def batchStep(self,data,targets,eta=1e-3):\n",
    "        '''Make one step of batch gradient descent'''                      \n",
    "        gw,gb,e = self.batchGradients(data,targets)\n",
    "        for i in range(len(gw)):\n",
    "            self.w[i] -= eta*gw[i]\n",
    "            self.biases[i] -= eta*gb[i]\n",
    "        return e\n",
    "       \n",
    "    def train(self,data,targets,eta=1e-4):\n",
    "        '''Train the network using batch GD'''\n",
    "        es = np.zeros(100000)\n",
    "        for i in range(len(es)):\n",
    "            es[i] = self.batchStep(data,targets,eta)\n",
    "            if i>100 and es[i-1]< es[i]+1e-5:\n",
    "                return es[:i+1]\n",
    "        return es"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question [10 credits]** Using the above code, create a neural network with no hidden layer and linear output activation function, and train it on the provided data. Plot the evolution of the training error, as well as the function that the neural network implements (in two separate plots). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Question [30 credits] ** Use the provided test data to compare this to other configurations of your choice (more hidden layers, different activation functions, ...). Find a network that performs well on the given data. Perform multiple training runs with random initialisations, plot the final validation error for each run in function of the number of nodes, and keep the best. Plot the train and test data, as well as the function implemented by your final network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Answer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
