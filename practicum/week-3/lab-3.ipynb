{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| | |\n",
    "|---|---|\n",
    "| **Name** | Jan Ubbo van Baardewijk & Thomas Brus |\n",
    "| **Group** | ML_HMI_01 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3: Training and testing\n",
    "\n",
    "In this lab we deal with overfitting and generalisation. We illustrate how these occur by extending last week's lab which was implementing logistic regression using basis functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression with base functions\n",
    "\n",
    "$\\newcommand{\\x}{\\mathbf{x}}\\newcommand{\\w}{\\mathbf{w}}\\newcommand{\\c}{\\mathcal{C}}\\newcommand{\\feat}{\\boldsymbol{\\phi}}$\n",
    "\n",
    "As we have seen last week, the logistic regression models the probability of a class label $\\c$ given a datapoint $p(\\c|\\x)$ as:\n",
    "$$p(\\c|\\x) = \\sigma(\\w^\\top\\x) = \\frac{1}{1+e^{-\\w^\\top\\x}}$$\n",
    "\n",
    "In this lab, we shall use basis functions to make this model non-linear in $\\x$, while still remaining linear in $\\w$, by replacing $\\x$ with a fixed, non-linear feature vector function $\\feat = f(\\x)$, so that $p(\\c|\\x)=\\sigma(\\w^\\top\\feat)$\n",
    "\n",
    "## Data\n",
    "\n",
    "We start by generating a dataset. As we have seen in the lecture, the central problem of machine learning is to devise algorithms that will generalise to previously unseen data, *i.e.*, that will correctly consider the distribution of the data-generating process into account, rather than merely the distribution of the training data. As you can imagine, this is more difficult for small training sets. The dataset in \"data-2class-nonlin\" is very similar to the data that we have used before but smaller and, therefore, harder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This code is given\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import pylab\n",
    "import copy\n",
    "pylab.rcParams['figure.figsize'] = (10.0, 8.0)\n",
    "\n",
    "maxexp=40 # Floats cannot represent exp(maxexp) and larger\n",
    "\n",
    "def sigma(a):\n",
    "    '''Logistic function, avoiding warnings of numerical overflows'''\n",
    "    if a>maxexp:\n",
    "        return 1.\n",
    "    if a<-maxexp:\n",
    "        return 0.\n",
    "    return 1./(1.+np.exp(-a))\n",
    "def log1pe(a):\n",
    "    '''Numerically accurate implementation of log(1+e^x)'''\n",
    "    if a<-maxexp:\n",
    "        return 0.\n",
    "    if a>maxexp:\n",
    "        return a\n",
    "    return np.log(1.+np.exp(a))\n",
    "def logsigma(a):\n",
    "    '''Numerically accurate log(sigma(x))'''\n",
    "    return -log1pe(-a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This code is given \n",
    "\n",
    "\n",
    "data = np.load(\"data-2class-nonlin.npz\")\n",
    "d = data['d']\n",
    "l = data['l']\n",
    "\n",
    "colours = [ [ 'b', 'r' ][int(x)] for x in np.nditer(l)] \n",
    "\n",
    "plt.scatter(d[:,0],d[:,1],20,colours)\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basis functions\n",
    "\n",
    "From the plot, it seems that we can do better than simply use a linear classifier. How about using a polynomial function as a discriminant? What about a quadratic function? Or a third-order polynomial?\n",
    "\n",
    "Such a polynomial function is a function of the type $y = w_0 + w_1 x + w_2 x^2 + w_3 x^3$, *etc.*. Notice how, in the previous equation, y is a non-linear function of x _but a linear function of $\\w$_. Optimising the function with respect to $\\w$ is still just as easy as last week. In actual fact, we are interested in a function of $x_1$ and $x_2$ rather than only $x$, but the principle is the same: $f(\\x,\\w)$ is still a linear function of $\\w$. \n",
    "\n",
    "**Question 1 [5 credits]**: Implement a Python function that takes in $\\x$ and the order of the polynomial, and returns the feature vector $\\feat$. For example `feat([x1,x2],2)` returns `[1,x1,x2,x1*x1,x1*x2,x2*x2]`. Bonus points for not including both `x1*x2` *and* `x2*x1` in the answer ;-).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#### Your answer to Q1 comes here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 3, 6, 10, 15, 21, 28, 36, 45, 55, 66, 78, 91, 105, 120, 136, 153, 171, 190, 210]\n"
     ]
    }
   ],
   "source": [
    "# This code is given\n",
    "# Have an array storing the dimensionality of the different models\n",
    "\n",
    "dims = [ len(feat([0,0],o)) for o in range(20)]\n",
    "print dims "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent and plotting the probability \n",
    "\n",
    "Here is some code for visualising the probability of the class given the input and for performing the gradient descent on the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This code is given\n",
    "\n",
    "xx,yy = np.mgrid[-5:10:.1,-5:10:.1]\n",
    "\n",
    "def plotHeat(w, order):\n",
    "    p = np.zeros(xx.shape)\n",
    "    for i in range(xx.shape[0]):\n",
    "        for j in range(xx.shape[1]):\n",
    "            p[i,j] = sigma(w.dot(feat(np.array([xx[i,j], yy[i,j]]), order)))\n",
    "    plt.pcolor(xx,yy,p,cmap='seismic')\n",
    "    plt.xlim([-5,10])\n",
    "    plt.ylim([-5,10])\n",
    "    plt.title(\"Order \"+str(order))\n",
    "\n",
    "def plotData():\n",
    "    plt.scatter(d[:,0],d[:,1],20,colours)\n",
    "    \n",
    "w = np.array([0,1,1,1,1,1])\n",
    "plotHeat(w,2)\n",
    "plotData()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This code is given\n",
    "\n",
    "def errorfun(w, data, labels):\n",
    "    '''Numerically accurate implementation of the error function'''\n",
    "    logprob = 0\n",
    "    for x,c in zip(data,labels):\n",
    "        logprob += log1pe(-w.dot(x)) + (1.-c)*w.dot(x)\n",
    "\n",
    "    return logprob\n",
    "\n",
    "def optimiseDisc(w, data, labels, order, eta=1e-3, maxrun=10000):\n",
    "    '''Implementation of batch gradient descent with adaptive step size'''\n",
    "    newError = errorfun(w,data,labels)\n",
    "    pastError = newError + 10\n",
    "    es = []\n",
    "    n=0\n",
    "    while pastError-newError > 1e-5 and n<maxrun:\n",
    "        n+=1\n",
    "        grad = np.zeros(w.shape)\n",
    "        for x,c in zip(data,labels):\n",
    "            grad += (sigma(w.dot(x)) - c) * x\n",
    "        pw = copy.copy(w)\n",
    "        w -= eta * grad\n",
    "        \n",
    "        pastError = newError\n",
    "        newError = errorfun(w,data,labels)\n",
    "        while eta > 0. and pastError - newError < 0: # If the error increases, eta is too large\n",
    "            eta /= 20                                # halve it, and\n",
    "            w = pw - eta * grad                      # try again from the original value of the weights\n",
    "            newError = errorfun(w,data,labels)\n",
    "        else:                                        # If the error goes down,\n",
    "            eta *= 1.2                               # try to increase eta a little, to speed up things.\n",
    "        \n",
    "        if newError<pastError:\n",
    "            es.append(newError)\n",
    "        else:                                        # If we couldn't decrease the error anymore, \n",
    "            break                                    # just give up.\n",
    "#        print n, pastError, newError, pastError-newError, eta\n",
    "\n",
    "    return w, newError\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Playground for using gradient descent..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generalisation\n",
    "\n",
    "Evaluate the error on the training set of gradient descent, with different orders of your feature vector. Pay attention to the evolution of the error function during the training; if you notice that the error goes up at times, your step size $\\eta$ is too large and you should reduce it. This error should keep diminishing: if it doesn't, you either stopped training too soon, or (for higher-order functions) you got stuck in a local optimum.\n",
    "\n",
    "Local optima are a problem for complex models. Sometimes, a complex model really is warranted, but finding optimal parameter values for it is difficult. In general, one solution is to initialise the parameters at random, run multiple optimisation runs, and keep the run with the best results. \n",
    "\n",
    "**Question 2 [10 credits]**: In this case, where there is a clear relationship between the parameters of the models of increasing complexity, can you think of a way to ensure that increasing the complexity of the model is guaranteed to reduce the training error (and so that, even if you get stuck in a local optimum, you're at least certain that it is an optimum that's better than the best model you've found so far? Does this strategy have disadvantages?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer to Q2*\n",
    "* ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "**Question 3 [10 credits]**: Divide your data into a training and test set. Plot the evolution of the error on both the train and the test set in function of the order of the polynomial.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Your answer to Q3 comes here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Question 4 [10 credits]**: Now perform 5-fold cross-validation on the data set and create the same plot as in the previous question. Doing cross-validation gives you a more accurate and stable estimate of the error of your model on unseen data, but it also gives you an idea of the spread of this error and therefore a (optimistic) idea of the upper and lower bounds on your error. Use error bars to show the standard deviation on your cross-validation error.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Your answer to Q4 comes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 5 [5 credits]**: What order should the polynomial be, do you think? Why do you think that?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer to Q5*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
