{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Names and group number\n",
    "\n",
    "ML_HMI_01: Jan Ubbo van Baardewijk & Thomas Brus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Discriminants: logistic regression\n",
    "\n",
    "$\\newcommand{\\x}{\\mathbf{x}}\\newcommand{\\w}{\\mathbf{w}}\\newcommand{\\c}{\\mathcal{C}}$\n",
    "\n",
    "As we have seen in the lecture, the logistic regression models the probability of a class label $\\c$ given a datapoint $p(\\c|\\x)$ as:\n",
    "$$p(\\c|\\x) = \\sigma(\\w^\\top\\x) = \\frac{1}{1+e^{-\\w^\\top\\x}}$$\n",
    "\n",
    "In this lab, we shall implement this model.\n",
    "\n",
    "## Logistic function\n",
    "\n",
    "To get started, let us plot the logistic function $\\sigma(a)$ in the range $[-5\\dots 5]$ in blue, and its derivative $\\frac{\\partial}{\\partial a} \\sigma(a)$ in green."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1097558d0>]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEACAYAAABI5zaHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x109755910>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.arange(-10, 10, .1)\n",
    "\n",
    "def sigma(a):\n",
    "    return 1.0 / (1.0 + np.exp(-a))\n",
    "\n",
    "plt.plot(x, sigma(x))\n",
    "plt.plot(x, sigma(x) * (1 - sigma(x)), 'green')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training data\n",
    "\n",
    "Next, load the data in the provided data file \"*data-2class.npz*\", and plot it. Then also plot the discriminant of a logistic regression model with parameters $\\w = [ 0,1,1]$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{1}{1+e^{\\w^\\top\\x}} = 1/2$, so $\\exp \\w^\\top \\x = 1$ or $\\w^\\top\\x = 0$. In other words, we can plot \n",
    "$w_0+w_1x_1 + w_2x_2 = 0$, or $x_1 = -\\frac{w_2x_2+w_0}{w_1}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data(filename):\n",
    "    data = np.load(filename)    \n",
    "    return (data['d'], data['l'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepend_ones(inputs):\n",
    "    # Extend the inputs by prepending a 1 for every row\n",
    "    vectors = np.ones((inputs.shape[0], inputs.shape[1] + 1))\n",
    "    vectors[:, 1:] = inputs\n",
    "    return vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "global name 'l' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-f0d690bd7ef0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvalid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvalid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mplot_scatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data-2class.npz\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0mplot_disc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-f0d690bd7ef0>\u001b[0m in \u001b[0;36mplot_scatter\u001b[0;34m(inputs, colors)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mplot_scatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'b'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcolors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnditer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mplot_disc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"black\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: global name 'l' is not defined"
     ]
    }
   ],
   "source": [
    "def plot_scatter(inputs, colors=['r', 'b']):\n",
    "    plt.scatter(inputs[:, 0], inputs[:, 1], 20, [colors[int(x)] for x in np.nditer(l)])\n",
    "\n",
    "def plot_disc(weights, color=\"black\"):\n",
    "    if abs(weights[1]) > abs(weights[2]):\n",
    "        yr = np.arange(-6, 10, 1)\n",
    "        xr = -(weights[2] * yr + weights[0]) / w[1]\n",
    "        valid  = (xr > -4) & (xr < 10)\n",
    "        plt.plot(xr[valid], yr[valid], color=color)\n",
    "    else:\n",
    "        xr = np.arange(-4, 10, 1)\n",
    "        yr = -(weights[1] * xr + weights[0]) / weights[2]\n",
    "        valid  = (yr > -6) & (yr <10)\n",
    "        plt.plot(xr[valid], yr[valid], color=color)\n",
    "\n",
    "plot_scatter(load_data(\"data-2class.npz\")[0])\n",
    "plot_disc([0, 1., 1.])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # Probability of the given labels for all datapoints\n",
    "\n",
    "We will now train our logistic regression model on a training dataset. This dataset contains a number of datapoints $\\x_1,\\dots,\\x_N$belonging to two classes and, for each datapoint $\\x_n$, the corresponding class label $l_n$. The training is done by *maximum likelihood*, that is, we maximise the probability of the data given the model. \n",
    "\n",
    "## Model\n",
    "\n",
    "Before we start coding, let's have a closer look at the model. Our labels are $0$ or $1$, representing the two classes. If a datapoint $\\x_n$ belongs to class $\\c_n=1$, the probability $p(\\c_n=1|\\x_n) = \\sigma(\\w^\\top\\x_n)$ and the probability $p(\\c_n=0|\\x_n) = 1-\\sigma(\\w^\\top\\x)$. We now want the probability of the labels given the data, for the complete training set: $p(\\c_1,\\dots,\\c_N|\\x_1,\\dots,\\x_N)$. Remember that the probability of the union of two independent events is $p(a,b)=p(a)\\,p(b)$. Using the fact that the labels are $0$ and $1$ (and that $x^0=1$ and $x^1=x$, we can then write this as \n",
    "$$p(\\c_1\\dots\\c_n|\\x_1\\dots\\x_n,\\w) = \\prod_{n=1}^N \\sigma(\\w^\\top\\x_n)^{\\c_n}\\,(1-\\sigma(\\w^\\top\\x_n))^{1-\\c_n}$$\n",
    "\n",
    "**Question [5 Marks]:** For the provided dataset, compute this probability when the model weights are $\\w = [ 0,-1,-1]$\n",
    "Think about how you could find the maximum of this function. What does the gradient of this probability look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "probability = 1.0\n",
    "weights = np.matrix([[0, -1, -1]])\n",
    "\n",
    "inputs, labels = load_data('data-2class.npz')\n",
    "vectors = prepend_ones(inputs)\n",
    "\n",
    "for x, c in zip(vectors, labels):\n",
    "    s = sigma(sum([a * b for a,b in zip(weights.flat, x)]))\n",
    "    probability *= s**c[0] * (1 - s)**(1 - c[0])\n",
    "    \n",
    "print \"Probability is (pretty much zero): \", probability\n",
    "\n",
    "print \"The maximum of the function is where the gradient is zero.\"\n",
    "print \"The gradient looks like the one plotted in the graph above,\"\n",
    "print \"but depending on the class it may be inverted vertically.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dealing with the joint probability is complicated, as 1) its value can become very small, easily too small for a computer to represent it accurately, and 2) taking the gradient of this joint probability with respect to $\\w$ makes the contribution of every datapoint to the gradient depend on every other datapoint's value.\n",
    "\n",
    "# Gradient\n",
    "\n",
    "So instead, we will define an error function which is the natural logarithm of the likelihood.\n",
    "\n",
    "**Question [5 marks]:** Write down the equation for $E(\\w) \\triangleq -\\log p(\\c_1,\\dots,\\c_N|\\x_1,\\dots,\\x_N)$ in analytical form.\n",
    "\n",
    "$$E(\\w) \\triangleq - \\sum_{n=1}^N \\{ C_n \\log(\\sigma(\\w^\\top\\x_n)) + (1 - C_n) \\log(1 - \\sigma(\\w^\\top\\x_n)) \\}$$\n",
    "\n",
    "**Question [5 marks]:** Compute the actual value of the error function for the given dataset and weight $w=[0,-1,-1]$.\n",
    "\n",
    "$$5056.86191856$$\n",
    "\n",
    "\n",
    "Notice that this function does not contain products of probabilities anymore, thus making the derivative of the joint probability with respect of each of the weights a sum of derivatives of the probabilities of single datapoints. The logarithm is what's called a \"monotonically increasing function\", which means that if $x_1 > x_2$, $f(x_1) > f(x_2)$. Because of that, the value of $\\w$ for which $p(\\c_1\\dots\\c_n|\\x_1\\dots\\x_n,\\w)$ is maximal is also the value of $\\w$ for which $\\log p(\\c_1\\dots\\c_n|\\x_1\\dots\\x_n,\\w)$ is maximal and thus $-\\log p(\\c_1\\dots\\c_n|\\x_1\\dots\\x_n,\\w)$ is minimal.\n",
    "\n",
    "The gradient of $\\nabla_\\w E(\\w)$ is given by $\\nabla_\\w E(\\w) = \\sum_{n=1}^N \\left(\\sigma( \\w^\\top\\x_n)-c_n\\right)\\x_n$\n",
    "\n",
    "**Question [30 marks]:** Implement gradient descent, updating the weights as $\\w^{\\mathrm{new}} = \\w^{\\mathrm{old}} - \\eta \\nabla_\\w E(\\w)$. The value of $\\eta$ will affect how fast the algorithm converges and is really up to you. In this case, I suggest you use $\\eta=10^{-3}$, which is quite small but allows you to see what's happening. This is a form of batch gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Question [5 marks]\n",
    "from math import log\n",
    "\n",
    "weights = np.matrix([[0, -1, -1]])\n",
    "\n",
    "inputs, labels = load_data('data-2class.npz')\n",
    "vectors = prepend_ones(inputs)\n",
    "\n",
    "logError = 0\n",
    "\n",
    "for x, c in zip(vectors, labels):\n",
    "    s = sigma(sum([a * b for a,b in zip(weights.flat, x)]))\n",
    "    logError += c[0] * log(s) + (1 - c[0]) * log(1 - s)\n",
    "\n",
    "print -logError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Question [30 marks]\n",
    "def calculate_error_gradients(inputs, labels, weights=weights):\n",
    "    vectors = prepend_ones(inputs)\n",
    "    error_gradients = [0.0] * (vectors.shape[1])\n",
    "\n",
    "    for x, c in zip(vectors, labels):\n",
    "        s = sigma(sum([a * b for a,b in zip(weights, x)]))\n",
    "        error_gradients += (s - c[0]) * x\n",
    "    \n",
    "    return error_gradients\n",
    "\n",
    "def update_weights(weights, error_gradients=[], learning_rate=0.001):\n",
    "    for i, weight in enumerate(weights):\n",
    "        weights[i] = weight - learning_rate * error_gradients[i]\n",
    "\n",
    "def gradient_descent(inputs, labels, learning_rate=0.001, threshold=1.0, max_iterations=500):\n",
    "    weights = [0.0] * (inputs.shape[1] + 1)\n",
    "    \n",
    "    for _ in range(max_iterations):\n",
    "        error_gradients = calculate_error_gradients(inputs, labels, weights=weights)        \n",
    "        update_weights(weights, error_gradients=error_gradients, learning_rate=learning_rate)\n",
    "        # Break early if error threshold is reached\n",
    "        if (np.mean(map(abs, error_gradients)) < threshold): break            \n",
    "    \n",
    "    return weights\n",
    "\n",
    "inputs, labels = load_data('data-2class.npz')\n",
    "weights = gradient_descent(inputs, labels)\n",
    "\n",
    "plot_scatter(inputs)\n",
    "plot_disc(weights)\n",
    "\n",
    "print weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualising the classifier\n",
    "\n",
    "It is important to realise that the classifier must be able to compute the probability of a label for any new datapoint, independently of whether it was in the training data or not. We can, therefore, plot what the probability of class $\\c_0$ is at any location in the input space. \n",
    "\n",
    "**Question [10 marks]:** Create a function that plots the data in a 3D graph in the plane $z=1/2$ and superimpose a wireframe of the probability of $\\c_0$. Then adapt your code in the previous question to call this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "import pylab\n",
    "pylab.rcParams['figure.figsize'] = (10.0, 8.0)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "data = np.load(\"data-2class.npz\")\n",
    "\n",
    "d = data['d']\n",
    "l = data['l'] # .sum(-1)\n",
    "\n",
    "xs = d[:, 0]\n",
    "ys = d[:, 1]\n",
    "\n",
    "# 3d scatter plot\n",
    "colors = [['r', 'b'][int(x)] for x in np.nditer(l)] \n",
    "ax.scatter(xs, ys, zs=0.5, zdir='z', s=20, c=colors, depthshade=False)\n",
    "\n",
    "# 3d wireframe plot\n",
    "x1 = np.arange(-10.0, 10.0, 1.0)\n",
    "y1 = np.arange(-10.0, 10.0, 1.0)\n",
    "\n",
    "weights = [-7.3483324916765183, 0.84189908664921853, 1.7161143830198393]\n",
    "\n",
    "X1, Y1 = np.meshgrid(x1, y1)\n",
    "\n",
    "def z(x, y):\n",
    "    return 1 - sigma(sum([a * b for a, b in zip(weights, [1, x, y])]))\n",
    "\n",
    "Z1 = z(X1, Y1)\n",
    "\n",
    "ax.plot_wireframe(X1, Y1, Z1, colors='green')\n",
    "\n",
    "# Why is this not working ...? Plot isn't quite right"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic gradient descent\n",
    "\n",
    "**Question [20 marks]:** As a last question, implement stochastic gradient descent and compare it to batch gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Question [30 marks]\n",
    "def calculate_error_gradients(input, label, weights=weights):\n",
    "    s = sigma(weights[0] + sum([a * b for a,b in zip(weights[1:], input)]))\n",
    "    return (s - label[0]) * input\n",
    "\n",
    "def update_weights(weights, error_gradients=[], learning_rate=0.001):\n",
    "    for i, weight in enumerate(weights):\n",
    "        weights[i] = weight - learning_rate * error_gradients[i]\n",
    "\n",
    "def gradient_descent(inputs, labels, learning_rate=0.001, threshold=1.0, max_iterations=5):\n",
    "    weights = [0.0] * (inputs.shape[1] + 1)\n",
    "    \n",
    "    for _ in range(max_iterations):\n",
    "        for x, c in zip(inputs, labels):\n",
    "            error_gradients = calculate_error_gradients(x, c, weights=weights)\n",
    "            update_weights(weights, error_gradients=error_gradients, learning_rate=learning_rate)\n",
    "            # Break early if error threshold is reached\n",
    "            if (np.mean(map(abs, error_gradients)) < threshold): break            \n",
    "    \n",
    "    return weights\n",
    "\n",
    "inputs, labels = load_data('data-2class.npz')\n",
    "weights = gradient_descent(inputs, labels)\n",
    "\n",
    "print weights\n",
    "\n",
    "# plot_scatter(inputs)\n",
    "# plot_disc(weights)\n",
    "\n",
    "# print weights\n",
    "\n",
    "# ...? Unable to figure out how to get this python magic working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
